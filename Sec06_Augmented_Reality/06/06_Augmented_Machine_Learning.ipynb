{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Project # 8  â€“ Augmented Reality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2, numpy as np, os\n",
    "\n",
    "#parameters\n",
    "working_dir = './'\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "org, font, scale, color, thickness, linetype = (50,50), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (234,12,123), 2, cv2.LINE_AA\n",
    "#chromakey values\n",
    "h,s,v,h1,s1,v1 = 16,0,64,123,111,187 #green\n",
    "h,s,v,h1,s1,v1 = 0,74,53,68,181,157 #skin tone\n",
    "#amount of data to use\n",
    "data_size = 1000\n",
    "#ratio of training data to test data\n",
    "training_to_test = .75\n",
    "#amount images are scaled down before being fed to keras\n",
    "img_size = 100\n",
    "#image height and width (from the webcam\n",
    "height, width = 480,640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#returns the region of interest around the largest countour\n",
    "#the accounts for objects not being centred in the frame\n",
    "def bbox(img):\n",
    "    try:\n",
    "        bg = np.zeros((1000,1000), np.uint8)\n",
    "        bg[250:250+480, 250:250+640] = img\n",
    "        _, contours, _  = cv2.findContours(img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        largest_contour = max(contours, key = cv2.contourArea)\n",
    "        rect = cv2.boundingRect(largest_contour)\n",
    "        circ = cv2.minEnclosingCircle(largest_contour)\n",
    "        x,y,w,h = rect\n",
    "        x,y = x+w/2,y+h/2\n",
    "        x,y = x+250, y+250\n",
    "        ddd = 200\n",
    "        return bg[y-ddd:y+ddd, x-ddd:x+ddd]\n",
    "    except: return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#finds the largest contour in a list of contours\n",
    "#returns a single contour\n",
    "def largest_contour(contours):\n",
    "    c = max(contours, key=cv2.contourArea)\n",
    "    return c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#finds the center of a contour\n",
    "#takes a single contour\n",
    "#returns (x,y) position of the contour\n",
    "def contour_center(c):\n",
    "    M = cv2.moments(c)\n",
    "    try: center = int(M['m10']/M['m00']), int(M['m01']/M['m00'])\n",
    "    except: center = 0,0\n",
    "    return center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#takes image and range\n",
    "#returns parts of image in range\n",
    "def only_color(img, (h,s,v,h1,s1,v1)):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    lower, upper = np.array([h,s,v]), np.array([h1,s1,v1])\n",
    "    mask = cv2.inRange(hsv, lower, upper)\n",
    "    kernel = np.ones((15,15), np.uint)\n",
    "    #mask - cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "    res = cv2.bitwise_and(img, img, mask=mask)\n",
    "    return res, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(dimData, images):\n",
    "    images = np.array(images)\n",
    "    images = images.reshape(len(images), dimData)\n",
    "    images = images.astype('float32')\n",
    "    images /=255\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------get train/test data-----------------\n",
    "images, labels = [],[]\n",
    "#iterate through tools\n",
    "tool_name = ''\n",
    "patterns = []\n",
    "tool_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    _, img = cap.read()\n",
    "    cv2.putText(img, 'enter class name (then enter)', org, font, scale, color, thickness, linetype)\n",
    "    cv2.putText(img, 'press esc when finished', (50,100), font, scale, color, thickness, linetype)\n",
    "    cv2.putText(img, tool_name, (50,300), font, 3, (0,0,255), 5, linetype)\n",
    "    \n",
    "    cv2.line(img, (330,240), (310,240), (234,123,234), 3)\n",
    "    cv2.line(img, (320,250), (320,230), (234,123,234), 3)\n",
    "    cv2.imshow('img', img)\n",
    "    k = cv2.waitKey(1)\n",
    "    if k>10: tool_name += chr(k)\n",
    "    if k == 27: break\n",
    "    #if tool name has been entered, start collecting the data\n",
    "    current = 0\n",
    "    if k == 13:\n",
    "        while current < data_size:\n",
    "            _, img = cap.read()\n",
    "            img, mask = only_color(img, (h,s,v,h1,s1,v1))\n",
    "            mask = bbox(mask)\n",
    "            images.append(cv2.resize(mask, (img_size, img_size)))\n",
    "            labels.append(tool_num)\n",
    "            current += 1\n",
    "            cv2.line(img, (330,240), (310,240), (234,123,234), 3)\n",
    "            cv2.line(img, (320,250), (320,230), (234,123,234), 3)\n",
    "            cv2.putText(img, 'collecting data', org, font, scale, color, thickness, linetype)\n",
    "            cv2.putText(img, 'data for'+tool_name+':' + str(current), (50,100), font, scale, color, thickness, linetype)\n",
    "            #cv2.imshow('img', img)\n",
    "            cv2.imshow('img', mask)\n",
    "            k = cv2.waitKey(1)\n",
    "            if k == ord('p'): cv2.waitKey(0)\n",
    "            if current == data_size:\n",
    "                patterns.append(tool_name)\n",
    "                tool_name = ''\n",
    "                tool_num += 1\n",
    "                \n",
    "                print tool_num\n",
    "                break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#break data into training and test sets\n",
    "to_train= 0\n",
    "train_images, test_images, train_labels, test_labels = [],[],[],[]\n",
    "for image, label in zip(images, labels):\n",
    "    if to_train<3:\n",
    "        train_images.append(image)\n",
    "        train_labels.append(label)\n",
    "        to_train+=1\n",
    "    else:\n",
    "        test_images.append(image)\n",
    "        test_labels.append(label)\n",
    "        to_train = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#-----------------keras time --> make the model\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "#flatten data\n",
    "dataDim = np.prod(images[0].shape)\n",
    "train_data  = flatten(dataDim, train_images)\n",
    "test_data = flatten(dataDim, test_images)\n",
    "\n",
    "#change labels to categorical\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "train_labels_one_hot = to_categorical(train_labels)\n",
    "test_labels_one_hot = to_categorical(test_labels)\n",
    "\n",
    "#determine the number of classes\n",
    "classes = np.unique(train_labels)\n",
    "nClasses  = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation = 'relu', input_shape = (dataDim,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nClasses, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2250 samples, validate on 750 samples\n",
      "Epoch 1/50\n",
      "2250/2250 [==============================] - 1s 495us/step - loss: 0.0586 - acc: 0.9836 - val_loss: 0.0153 - val_acc: 0.9973\n",
      "Epoch 2/50\n",
      "2250/2250 [==============================] - 1s 244us/step - loss: 0.0066 - acc: 0.9973 - val_loss: 0.0119 - val_acc: 0.9973\n",
      "Epoch 3/50\n",
      "2250/2250 [==============================] - 1s 295us/step - loss: 0.0048 - acc: 0.9987 - val_loss: 0.0092 - val_acc: 0.9973\n",
      "Epoch 4/50\n",
      "2250/2250 [==============================] - 1s 265us/step - loss: 0.0035 - acc: 0.9987 - val_loss: 0.0280 - val_acc: 0.9960\n",
      "Epoch 5/50\n",
      "2250/2250 [==============================] - 1s 250us/step - loss: 0.0018 - acc: 0.9996 - val_loss: 0.0162 - val_acc: 0.9973\n",
      "Epoch 6/50\n",
      "2250/2250 [==============================] - 1s 260us/step - loss: 0.0029 - acc: 0.9987 - val_loss: 0.0163 - val_acc: 0.9973\n",
      "Epoch 7/50\n",
      "2250/2250 [==============================] - 1s 257us/step - loss: 0.0012 - acc: 0.9996 - val_loss: 0.0131 - val_acc: 0.9973\n",
      "Epoch 8/50\n",
      "2250/2250 [==============================] - 1s 250us/step - loss: 0.0015 - acc: 0.9996 - val_loss: 0.0060 - val_acc: 0.9987\n",
      "Epoch 9/50\n",
      "2250/2250 [==============================] - 1s 248us/step - loss: 0.0023 - acc: 0.9991 - val_loss: 0.0076 - val_acc: 0.9987\n",
      "Epoch 10/50\n",
      "2250/2250 [==============================] - 1s 284us/step - loss: 1.8922e-04 - acc: 1.0000 - val_loss: 0.0102 - val_acc: 0.9987\n",
      "Epoch 11/50\n",
      "2250/2250 [==============================] - 1s 275us/step - loss: 2.7390e-04 - acc: 1.0000 - val_loss: 0.0112 - val_acc: 0.9987\n",
      "Epoch 12/50\n",
      "2250/2250 [==============================] - 1s 274us/step - loss: 8.9939e-05 - acc: 1.0000 - val_loss: 0.0133 - val_acc: 0.9987\n",
      "Epoch 13/50\n",
      "2250/2250 [==============================] - 1s 270us/step - loss: 5.6692e-04 - acc: 0.9996 - val_loss: 0.0188 - val_acc: 0.9973\n",
      "Epoch 14/50\n",
      "2250/2250 [==============================] - 1s 261us/step - loss: 0.0011 - acc: 0.9996 - val_loss: 0.0262 - val_acc: 0.9973\n",
      "Epoch 15/50\n",
      "2250/2250 [==============================] - 1s 285us/step - loss: 2.2131e-04 - acc: 1.0000 - val_loss: 0.0228 - val_acc: 0.9973\n",
      "Epoch 16/50\n",
      "2250/2250 [==============================] - 1s 255us/step - loss: 2.0790e-04 - acc: 1.0000 - val_loss: 0.0193 - val_acc: 0.9973\n",
      "Epoch 17/50\n",
      "2250/2250 [==============================] - 1s 270us/step - loss: 6.3249e-04 - acc: 0.9996 - val_loss: 0.0227 - val_acc: 0.9973\n",
      "Epoch 18/50\n",
      "2250/2250 [==============================] - 1s 261us/step - loss: 1.1214e-05 - acc: 1.0000 - val_loss: 0.0227 - val_acc: 0.9973\n",
      "Epoch 19/50\n",
      "2250/2250 [==============================] - 1s 258us/step - loss: 6.2515e-05 - acc: 1.0000 - val_loss: 0.0200 - val_acc: 0.9973\n",
      "Epoch 20/50\n",
      "2250/2250 [==============================] - 1s 273us/step - loss: 3.2726e-04 - acc: 1.0000 - val_loss: 0.0214 - val_acc: 0.9973\n",
      "Epoch 21/50\n",
      "2250/2250 [==============================] - 1s 255us/step - loss: 6.7142e-06 - acc: 1.0000 - val_loss: 0.0163 - val_acc: 0.9987\n",
      "Epoch 22/50\n",
      "2250/2250 [==============================] - 1s 255us/step - loss: 1.8821e-05 - acc: 1.0000 - val_loss: 0.0188 - val_acc: 0.9987\n",
      "Epoch 23/50\n",
      "2250/2250 [==============================] - 1s 268us/step - loss: 2.0078e-04 - acc: 1.0000 - val_loss: 0.0316 - val_acc: 0.9973\n",
      "Epoch 24/50\n",
      "2250/2250 [==============================] - 1s 259us/step - loss: 2.9022e-05 - acc: 1.0000 - val_loss: 0.0160 - val_acc: 0.9987\n",
      "Epoch 25/50\n",
      "2250/2250 [==============================] - 1s 255us/step - loss: 0.0035 - acc: 0.9996 - val_loss: 0.0044 - val_acc: 0.9987\n",
      "Epoch 26/50\n",
      "2250/2250 [==============================] - 1s 257us/step - loss: 2.0942e-04 - acc: 1.0000 - val_loss: 0.0121 - val_acc: 0.9987\n",
      "Epoch 27/50\n",
      "2250/2250 [==============================] - 1s 260us/step - loss: 9.1237e-06 - acc: 1.0000 - val_loss: 0.0141 - val_acc: 0.9987\n",
      "Epoch 28/50\n",
      "2250/2250 [==============================] - 1s 261us/step - loss: 1.9014e-05 - acc: 1.0000 - val_loss: 0.0128 - val_acc: 0.9987\n",
      "Epoch 29/50\n",
      "2250/2250 [==============================] - 1s 253us/step - loss: 7.3872e-05 - acc: 1.0000 - val_loss: 0.0128 - val_acc: 0.9987\n",
      "Epoch 30/50\n",
      "2250/2250 [==============================] - 1s 262us/step - loss: 3.4314e-04 - acc: 0.9996 - val_loss: 0.0239 - val_acc: 0.9973\n",
      "Epoch 31/50\n",
      "2250/2250 [==============================] - 1s 257us/step - loss: 1.8239e-05 - acc: 1.0000 - val_loss: 0.0249 - val_acc: 0.9973\n",
      "Epoch 32/50\n",
      "2250/2250 [==============================] - 1s 258us/step - loss: 1.8084e-06 - acc: 1.0000 - val_loss: 0.0234 - val_acc: 0.9973\n",
      "Epoch 33/50\n",
      "2250/2250 [==============================] - 1s 257us/step - loss: 4.9311e-06 - acc: 1.0000 - val_loss: 0.0270 - val_acc: 0.9973\n",
      "Epoch 34/50\n",
      "2250/2250 [==============================] - 1s 275us/step - loss: 1.9728e-05 - acc: 1.0000 - val_loss: 0.0207 - val_acc: 0.9973\n",
      "Epoch 35/50\n",
      "2250/2250 [==============================] - 1s 260us/step - loss: 1.0054e-06 - acc: 1.0000 - val_loss: 0.0220 - val_acc: 0.9973\n",
      "Epoch 36/50\n",
      "2250/2250 [==============================] - 1s 264us/step - loss: 0.0022 - acc: 0.9991 - val_loss: 0.0345 - val_acc: 0.9973\n",
      "Epoch 37/50\n",
      "2250/2250 [==============================] - 1s 263us/step - loss: 1.4276e-06 - acc: 1.0000 - val_loss: 0.0273 - val_acc: 0.9973\n",
      "Epoch 38/50\n",
      "2250/2250 [==============================] - 1s 260us/step - loss: 2.7173e-05 - acc: 1.0000 - val_loss: 0.0235 - val_acc: 0.9973\n",
      "Epoch 39/50\n",
      "2250/2250 [==============================] - 1s 262us/step - loss: 3.1111e-06 - acc: 1.0000 - val_loss: 0.0364 - val_acc: 0.9973\n",
      "Epoch 40/50\n",
      "2250/2250 [==============================] - 1s 263us/step - loss: 2.8341e-06 - acc: 1.0000 - val_loss: 0.0377 - val_acc: 0.9973\n",
      "Epoch 41/50\n",
      "2250/2250 [==============================] - 1s 260us/step - loss: 1.6783e-06 - acc: 1.0000 - val_loss: 0.0262 - val_acc: 0.9973\n",
      "Epoch 42/50\n",
      "2250/2250 [==============================] - 1s 259us/step - loss: 1.1735e-05 - acc: 1.0000 - val_loss: 0.0299 - val_acc: 0.9973\n",
      "Epoch 43/50\n",
      "2250/2250 [==============================] - 1s 258us/step - loss: 2.7305e-07 - acc: 1.0000 - val_loss: 0.0290 - val_acc: 0.9973\n",
      "Epoch 44/50\n",
      "2250/2250 [==============================] - 1s 257us/step - loss: 2.2199e-05 - acc: 1.0000 - val_loss: 0.0307 - val_acc: 0.9973\n",
      "Epoch 45/50\n",
      "2250/2250 [==============================] - 1s 258us/step - loss: 4.4504e-07 - acc: 1.0000 - val_loss: 0.0299 - val_acc: 0.9973\n",
      "Epoch 46/50\n",
      "2250/2250 [==============================] - 1s 257us/step - loss: 1.2811e-05 - acc: 1.0000 - val_loss: 0.0390 - val_acc: 0.9973\n",
      "Epoch 47/50\n",
      "2250/2250 [==============================] - 1s 260us/step - loss: 1.8759e-07 - acc: 1.0000 - val_loss: 0.0388 - val_acc: 0.9973\n",
      "Epoch 48/50\n",
      "2250/2250 [==============================] - 1s 254us/step - loss: 1.3202e-06 - acc: 1.0000 - val_loss: 0.0430 - val_acc: 0.9973\n",
      "Epoch 49/50\n",
      "2250/2250 [==============================] - 1s 255us/step - loss: 9.7050e-07 - acc: 1.0000 - val_loss: 0.0430 - val_acc: 0.9973\n",
      "Epoch 50/50\n",
      "2250/2250 [==============================] - 1s 261us/step - loss: 4.7219e-04 - acc: 0.9996 - val_loss: 0.0217 - val_acc: 0.9987\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(train_data, train_labels_one_hot, batch_size = 256, epochs=50, verbose=1,\n",
    "                    validation_data=(test_data, test_labels_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 145us/step\n",
      "Evaluation result on Test Data : Loss = 0.0216675713807, accuracy = 0.998666666667\n"
     ]
    }
   ],
   "source": [
    "#test model\n",
    "[test_loss, test_acc] = model.evaluate(test_data, test_labels_one_hot)\n",
    "print(\"Evaluation result on Test Data : Loss = {}, accuracy = {}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_prediction(bg,prediction, motion):\n",
    "    idxs = [1,2,3,4,5,6,7,8,9]\n",
    "    for i, pattern, idx in zip(prediction, patterns, idxs):\n",
    "        text = pattern + ' '+str(round(i,3))\n",
    "        scale = i*2\n",
    "        \n",
    "        if motion: scale = .4\n",
    "        if scale<.95: scale = .95\n",
    "        thickness = 1\n",
    "        if scale>1.5: thickness = 2\n",
    "        if scale>1.95: thickness = 4\n",
    "        scale = scale*.75\n",
    "        org, font, color = (350, idx*70), cv2.FONT_HERSHEY_SIMPLEX, (0,0,0)\n",
    "        cv2.putText(bg, text, org, font, scale, color, thickness, cv2.LINE_AA)\n",
    "    return bg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_bg(prediction):\n",
    "    motion = False\n",
    "    bg = np.zeros((1150,1000,3), np.uint8)\n",
    "    idxs = [1,2,3,4,5,6,7,8,9]\n",
    "    for i, pattern, idx in zip(prediction, patterns, idxs):\n",
    "        text = pattern + ' '+str(round(i,3))\n",
    "        scale = i*2\n",
    "        \n",
    "        if motion: scale = .4\n",
    "        if scale<.95: scale = .95\n",
    "        thickness = 1\n",
    "        if scale>1.5: thickness = 2\n",
    "        if scale>1.95: thickness = 4\n",
    "        scale = scale*2\n",
    "        org, font, color = (200, idx*140), cv2.FONT_HERSHEY_SIMPLEX, (12,234,123)\n",
    "        cv2.putText(bg, text, org, font, scale, (255,255,255), 1+thickness, cv2.LINE_AA)\n",
    "    return bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "#model = load_model('/home/stephen/Desktop/hand_model.h5')\n",
    "dimData = np.prod([img_size, img_size])\n",
    "while True:\n",
    "    _, img= cap.read()\n",
    "    _, mask = only_color(img, (h,s,v,h1,s1,v1))\n",
    "    mask = bbox(mask)\n",
    "    mask = cv2.resize(mask, (img_size, img_size))\n",
    "    cv2.imshow('display', mask)\n",
    "    mask = mask.reshape(dimData)\n",
    "    mask = mask.astype('float32')\n",
    "    mask /=255\n",
    "    prediction = model.predict(mask.reshape(1,dimData))[0].tolist()\n",
    "    img = draw_prediction(img, prediction, False)\n",
    "    display = draw_bg(prediction)\n",
    "    \n",
    "    cv2.imshow('img', img)\n",
    "    k = cv2.waitKey(10)\n",
    "    if k == 27: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
